<html>
  <head>
    <title>Understanding 3D Object Articulation in Internet Videos</title>
    <meta property="og:title" content="Understanding 3D Object Articulation in Internet Videos"/>
    <meta property="og:image" content="teaser.png"/>
    <meta property="og:description" content="S. Qian, L. Jin, C. Rockwell, S. Chen, D. F. Fouhey.." />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <!-- webpage template-->
    <link rel="stylesheet" href="website.css">  
  </head>
  <body>
    <br>
    <center>
      <span style="font-size:38px">Understanding 3D Object Articulation in Internet Videos</span>
    </center>
    <br><br>
    <table align=center width=800px>
      <tr>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://jasonqsy.github.io/">Shengyi Qian</a></span>
          </center>
        </td>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://jinlinyi.github.io/">Linyi Jin</a></span>
          </center>
        </td>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://crockwell.github.io/">Chris Rockwell</a></span>
          </center>
        </td>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://chicychen.github.io/">Siyi Chen</a></span>
          </center>
        </td>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://web.eecs.umich.edu/~fouhey/">David F. Fouhey</a></span>
          </center>
        </td>
      </tr>
    </table>
    <br>
    <table align=center width=700px>
      <tr>
        <td align=center width=200px>
          <center>
            <span style="font-size:20px">University of Michigan</span>
          </center>
        </td>
      </tr>
    </table>
    <br>
    <table align=center width=700px>
      <tr>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px">CVPR 2022</span>
          </center>
        </td>
      </tr>
    </table>
    <br>
    <table align=center width=500px>
      <tr>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://arxiv.org/abs/2203.16531">[pdf]</a></span>
          </center>
        </td>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://github.com/JasonQSY/Articulation3D">[code]</a></span>
          </center>
        </td>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"><a href="https://www.youtube.com/watch?v=yUptXNEMS6g">[video]</a></span>
          </center>
        </td>
      </tr>
    </table>
    <table align=center width=700px>
      <tr>
        <td align=center width=100px>
          <center>
            <span style="font-size:20px"></span>
          </center>
        </td>
      </tr>
    </table>
    <br>
    <table align=center width=900px>
      <tr>
        <td width=00px>
          <center>
            <img src = "teaser.png" width="900px"></img><br>
          </center>
        </td>
      </tr>
      <td width=600px>
        <center>
          <span style="font-size:14px"><i>Given an ordinary video, our system produces a 3D planar representation of the observed articulation. The 3D renderings illustrate how the microwave (in Pink) can be articulated in 3D space. We also show the predicted rotation axis using a Blue arrow.</i>
        </center>
      </td>
      </tr>
    </table>
    <br>
    We propose to investigate detecting and characterizing the 3D planar articulation of objects from ordinary videos. While seemingly easy for humans, this problem poses many challenges for computers. We propose to approach this problem by combining a top-down detection system that finds planes that can be articulated along with an optimization approach that solves for a 3D plane that can explain a sequence of observed articulations. We show that this system can be trained on a combination of videos and 3D scan datasets. When tested on a dataset of challenging Internet videos and the Charades dataset, our approach obtains strong performance.
    <br><br>
    <hr>
    <table align=center width=1100px>
      <tr>
        <td>
          <left>
            <center>
              <h1>Video</h1>
            </center>
          </left>
        </td>
      </tr>
    </table>
    <center>
    <iframe width="690" height="388" src="https://www.youtube.com/embed/yUptXNEMS6g" title="Understanding 3D Object Articulation in Internet Videos" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </center>
    <br><br>
    <hr>
    <table align=center width=1100px>
      <tr>
        <td>
          <left>
            <center>
              <h1>Dataset</h1>
              <h3>Internet videos</h3>
            </center>
          </left>
        </td>
      </tr>
    </table>
    <table id="customers" align=center width=1000px>
      <tr>
          <th width=14%>Category</th>
          <th width=23%>Links</th>
          <th>Details</th>
      </tr>
      <tr>
          <td>Video Clips</td>
          <td><a href='https://drive.google.com/file/d/1TU6E7sUL4tv2d_C83au2xsAh8fT69V6n/view?usp=sharing'>pos_clips.tar.gz</a></td>
          <td>Articulation video clips. Each clip lasts 3 seconds.</td>
      </tr>
      <tr>
        <td>Negative Clips</td>
        <td><a href='https://drive.google.com/file/d/1S6o0VnaLl3o7z9ZinLvwbOZrWKe8FZp9/view?usp=sharing'>neg_clips.tar.gz</a></td>
        <td>For each positive video clip, we try to sample a negative clip (no articulation) in the same scene with a hand motion. This is used for the recogition benchmark.</td>
      </tr>
      <tr>
          <td>Frames</td>
          <td><a href='https://drive.google.com/file/d/1aO5C2WyD4BEZwihuS5x5XUMj99ztqQjb/view?usp=sharing'>articulation_frames_v1.tar.gz</a></td>
          <td>Key frames pre-extracted for the dataset. We extract 9 key frames for each video clip, which has 90 frames (fps=30).</td>
      </tr>
      <tr>
          <td>Annotations</td>
          <td><a href='https://drive.google.com/file/d/1fctqYS41hg9TW1d0mhq6eKQraZIUT1Pg/view?usp=sharing'>articulation_annotations_v1.tar.gz</a></td>
          <td>Articulation annotations. Surface normals are only available in the test split. We have preprocessed annotations to COCO format.</td>
      </tr>
    </table>
    <br><br>
    <table align=center width=1100px>
      <tr>
        <td>
          <left>
            <center>
              <h3>ScanNet</h3>
            </center>
          </left>
        </td>
      </tr>
    </table>
    <table id="customers" align=center width=1000px>
      <tr>
          <th width=14%>Category</th>
          <th width=23%>Links</th>
          <th>Details</th>
      </tr>
      <tr>
        <td>Annotations</td>
        <td><a href='https://drive.google.com/file/d/1AH98ghx7dFT8oycBEBwL0eAKSUMbv7aI/view?usp=sharing'>scannet_annotations.tar.gz</a></td>
        <td>ScanNet plane annotations. It is preprocessed by <a href="https://jinlinyi.github.io/SparsePlanes/">SparsePlanes</a>.</td>
      </tr>
      <tr>
        <td>SURREAL images</td>
        <td><a href='https://drive.google.com/file/d/1lqeunygM_bPkjXmauI5TCx8C8ZqwboK8/view?usp=sharing'>scannet_surreal_imgs.tar.gz</a></td>
        <td>We render synthetic humans on around 98k ScanNet images. You can extract it to the ScanNet folder.</td>
      </tr>
      <tr>
        <td>SURREAL annotations</td>
        <td><a href='https://drive.google.com/file/d/1dAQ3QF8rZUsR8x8K4CxA5AEMn5lQFega/view?usp=sharing'>scannet_surreal_annotations.tar.gz</a></td>
        <td>The same plane annotations but we change image path to SURREAL images.</td>
      </tr>
    </table>
    <br><br>
    <hr>
    <!-- <table align=center width=550px> -->
    <br><br>
    <table align=center width=1100px>
      <tr>
        <td>
          <left>
            <center>
              <h1>Acknowledgements</h1>
            </center>
            This work was supported by the DARPA Machine Common Sense Program and Toyota Research Institute. Toyota Research Institute (“TRI”) provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.
          </left>
        </td>
      </tr>
    </table>
    <!-- Loads <model-viewer> for modern browsers: -->
    <script type="module"
      src="https://unpkg.com/@google/model-viewer@v0.9.0/dist/model-viewer.js"></script>
    <!-- Loads <model-viewer> for old browsers like IE11: -->
    <script nomodule
      src="https://unpkg.com/@google/model-viewer@v0.9.0/dist/model-viewer-legacy.js"></script>
  </body>
</html>
